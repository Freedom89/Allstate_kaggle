{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.cross_validation import KFold\n",
    "from scipy.stats import skew, boxcox\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import itertools\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "train_x = pd.read_csv(\"../input/train_x_power2.csv\")\n",
    "train_y = pd.read_csv(\"../input/train_y_power2.csv\",header=None)\n",
    "ids = pd.read_csv(\"../input/ids.csv\")\n",
    "test_x = pd.read_csv(\"../input/test_power2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "shift = 200\n",
    "SEED = 2016\n",
    "def xg_eval_mae(yhat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(y)-shift,\n",
    "                                      np.exp(yhat)-shift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Df_results = pd.DataFrame() \n",
    "iter_count = 1\n",
    "partial_evalutaion = open('extra_trees_bootstrap2.txt','a') \n",
    "\n",
    "def objective(space):\n",
    "    \n",
    "    #n_folds = n_folds\n",
    "    log_files = space.copy()\n",
    "    \n",
    "    n_folds = 5\n",
    "    cv_sum = 0\n",
    "    \n",
    "    kf = KFold(train_x.shape[0], n_folds=n_folds,random_state = 2016)\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        print(i)\n",
    "\n",
    "        X_train, X_val = train_x.iloc[train_index], train_x.iloc[test_index]\n",
    "        y_train, y_val = train_y.iloc[train_index], train_y.iloc[test_index]\n",
    "\n",
    "        d_train, d_train_y = np.array(X_train), np.array(y_train[0])\n",
    "        d_valid, d_valid_y = np.array(X_val),np.array(y_val[0])\n",
    "\n",
    "        clf = ExtraTreesRegressor(n_estimators = space['n_estimators'],\n",
    "                                    max_features = space['max_features'],\n",
    "                                    max_depth = space['max_depth'],\n",
    "                                    min_samples_leaf = space['min_samples_leaf'],\n",
    "                                    n_jobs = 18,\n",
    "                                    criterion = 'mse',\n",
    "                                    random_state = 2016,\n",
    "                                    bootstrap = True,\n",
    "                                    verbose = 1)\n",
    "        \n",
    "        clf.fit(d_train,d_train_y)\n",
    "\n",
    "        scores_val = clf.predict(d_valid)\n",
    "\n",
    "        cv_score = mean_absolute_error(np.exp(d_valid_y), np.exp(scores_val))\n",
    "        print('eval-MAE: %.6f' % cv_score)\n",
    "        \n",
    "        cv_score_name = str(i) + '_cv_score' \n",
    "        rounds_name = str(i) + '_n_rounds' \n",
    "        \n",
    "        log_files[cv_score_name] = cv_score \n",
    "        cv_sum = cv_sum + cv_score\n",
    "\n",
    "    score = cv_sum / n_folds\n",
    "    #print 'the score for this round is :' + str(score )\n",
    "    \n",
    "    log_files['score'] = score\n",
    "    \n",
    "    log_files_df = pd.DataFrame(log_files.items(), columns = ['params','value'])\n",
    "\n",
    "    global iter_count \n",
    "    print '##################' + '   iteration ' + str(iter_count) + '    with' + ' ' + str(score) + ' ##############'\n",
    "    \n",
    "    partial_evalutaion.write('iteration ' + str(space) +str(iter_count) + 'with' + ' ' + str(score) + '\\n')\n",
    "    partial_evalutaion.flush()\n",
    "    \n",
    "    log_files_df['id'] = iter_count\n",
    "    iter_count = iter_count+1 \n",
    "    log_files_df = log_files_df.pivot(index = 'id',columns = 'params',values = 'value')\n",
    "    \n",
    "    global Df_results\n",
    "    Df_results = Df_results.append(log_files_df)\n",
    "    \n",
    "    return {'loss': score, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "space = {\n",
    "         'max_depth' : hp.choice('max_depth', np.arange(15, 25, dtype=int)),\n",
    "         'n_estimators' : hp.choice('n_estimators', np.arange(100,300,dtype=int)),\n",
    "         'max_features' : hp.quniform('max_features', 0.2, 1, 0.05),\n",
    "         'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(6,20,dtype=int))\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best = fmin(objective,space,algo = tpe.suggest,trials = trials, max_evals = 100)\n",
    "    \n",
    "print best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "Df_results.to_csv(\"hyperopt_ef.csv\",index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
